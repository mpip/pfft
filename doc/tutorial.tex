%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Tutorial}\label{chap:tuto}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following chapter describes the usage of the PFFT library corresponding to the simple test files,
that are included in the library. If you are interested in the more advanced features of PFFT you
should also have a look at chapter~\ref{chap:feat}.

\section{A first parallel transform}
We explain the basic steps for computing a parallel FFT with the PFFT library at the example
of the short test program given by Listing~\ref{lst:min_c2c}. This test computes a three-dimensional c2c-FFT on
a two-dimensional process mesh. The source code can also be found in directory \code{testsgc}
of the library's source code tree. 
\lstinputlisting[float, caption={Minimal parallel c2c-FFT test program.}, label=lst:min_c2c]{code/manual_min_c2c.c}

After initializing MPI with \code{MPI_Init} and before calling any PFFT routine we need to initialize
the parallel computations via
\begin{lstlisting}
void pfft_init(void);
\end{lstlisting}
MPI introduces the concept of communicators to store all the topological information of the physical process layout.
PFFT requires to be called on a process mesh that corresponds to a periodic, Cartesian communicator.
We assist the user in creating such a communicator with the following routine
\begin{lstlisting}
int pfft_create_procmesh_2d(
    MPI_Comm comm, int np0, int np1,
    MPI_Comm *comm_cart_2d);
\end{lstlisting}
This routine uses the processes within the communicator \code{comm} to create a two-dimensional process
grid of size \code{np0} x \code{np1} and stores it into the Cartesian communicator \code{comm\_cart\_2d}.
Since \code{comm\_cart\_2d} is allocated by the routine, the user has to provide
the pointer to a non-allocated communicator. Otherwise we end up with a memory leak.
Most users will choose \code{comm} as \code{MPI\_COMM\_WORLD}, which implies that the FFT is computed
on all available processes. Nevertheless, it's important to note, that we follow the principles of MPI
by allowing \code{comm} to be any arbitrary communicator and therefore any subset of all available processes.

At the next step we need to know the data decomposition of the input and output array, that depends on
the array sizes, the process grid and the chosen parallel algorithm. Therefore, we call
\begin{lstlisting}
ptrdiff_t pfft_local_size_3d(
    ptrdiff_t *n, MPI_Comm comm_cart_2d, unsigned pfft_flags,
    ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    ptrdiff_t *local_no, ptrdiff_t *local_o_start);
\end{lstlisting}
Hereby, \code{n}, \code{local_ni}, \code{local_i_start}, \code{local_no}, \code{local_o_start} are supposed
to be arrays of length $3$.
This function returns the size of the local complex array that needs to be allocated by every process to
execute PFFT. In most cases, this coincides with the product of the local array sizes, but in some cases
it may be bigger, whenever the parallel algorithm needs some extra storage to be executed.
The input value \code{n} consists of the three-dimensional FFT size and \code{pfft_flags} serves to adjust
some details of the parallel execution. For sake of simplicity, we restrict our self to
\code{pfft_flags=PFFT_TRANSPOSED_NONE} for a while and explain the more sophisticated flags at a later point.
The output arrays \code{local_ni} and \code{local_i_start} give the size and the offset of the local input array
that results from the parallel block distribution of the global input array, i.e.,
every process owns the input data \code{in[k[0],k[1],k[2]]} with \code{local_i_start[t] <= k[t] < local_i_start[t] + local_ni[t]}
for \code{t=0,1,2}. Analogously, the output parameters \code{local_i_start} and \code{local_no} contain the size
and the offset of the local output array.

Next the input and output arrays must be allocated. For performance reasons this should be done using
\begin{lstlisting}
pfft_complex* pfft_alloc_complex(size_t size);
\end{lstlisting}
Nevertheless, you can also use any other dynamic memory allocation. Have a look at Section~3.1
of the FFTW user manual available under \url{www.fftw.orggcfftw3_doc} for more details on alignment
and \code{fftw_malloc}, which is called within all PFFT allocation functions.

The planning of a single three-dimensional parallel FFT of size \code{n[0]} x \code{n[1]} x \code{n[2]}
is done by the function
\begin{lstlisting}
pfft_plan pfft_plan_dft_3d(
    ptrdiff_t *n, pfft_complex *in, pfft_complex *out,
    MPI_Comm comm_cart_2d, int sign, unsigned pfft_flags);
\end{lstlisting}
We provide the address of the input and output array by the pointers \code{in} and \code{out},
respectively. An inplace transform is assumed if these pointers are equal. 
Similar to FFTW the integer \code{sign} gives the sign in the exponential of the FFT.
The \code{pfft\_flags} must coincide with the flags that were used to call \code{pfft_local_size_3d}.
Otherwise the data layout of the parallel execution may not match calculated local array sizes.
This function returns a plan, some structure that includes all the information we need to perform a
parallel FFT.

Once the plan is generated, we are allowed to fill the input array \code{in}. Note, that in most cases
the planning \code{pfft_plan_dft_3d} would overwrite the input array \code{in}. That's you should not
write any sensitive data into \code{in} until the plan was generated.
For simplicity, our test program makes use of the library function
\begin{lstlisting}
void pfft_init_input_3d(
    ptrdiff_t *n, ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
    MPI_Comm comm_cart_2d,
    fftw_complex *in);
\end{lstlisting}
to fill the input array with some numbers. Alternatively, one can fill the array with a function \code{func} of choice
and the following loop that takes account of the parallel data layout
\begin{lstlisting}
ptrdiff_t m=0;
for(ptrdiff_t k0=0; k0 < local_ni[0]; k0++)
  for(ptrdiff_t k1=0; k1 < local_ni[1]; k1++)
    for(ptrdiff_t k2=0; k2 < local_ni[2]; k2++)
      in[m++] = func(k0 + local_i_start[0],
                     k1 + local_i_start[1],
                     k2 + local_i_start[2]);
\end{lstlisting}
The parallel FFT is computed at the moment we execute the generated plan
\begin{lstlisting}
void pfft_execute(pfft_plan plan);
\end{lstlisting}
Now, the results can be read from \code{out} with an analogous three-dimensional loop.
If we do not want to execute another parallel FFT of the same type, we free the allocated memory of the plan with
\begin{lstlisting}
void pfft_destroy_plan(pfft_plan plan);
\end{lstlisting}
Additionally, we use
\begin{lstlisting}
int MPI_Comm_free(MPI_Comm *comm);  
\end{lstlisting}
to free the communicator allocated by \code{pfft_create_procmesh_2d} and
\begin{lstlisting}
void pfft_free(void *ptr);
\end{lstlisting}
to free memory allocated by \code{pfft_alloc_complex}.
Finally, we exit MPI by
\begin{lstlisting}
int MPI_Finalize(void);
\end{lstlisting}

\section{More sophisticated options}

\subsection{Errorcode for communicator creation}
As we have seen the function
\begin{lstlisting}
int pfft_create_procmesh_2d(
    MPI_Comm comm, int np0, int np1,
    MPI_Comm *comm_cart_2d);
\end{lstlisting}
gives back an integer value, that has not been used in Listing~\ref{lst:min_c2c}.
This integer is an error code that is equal to zero as long as the communicator was created
without any errors. The most common error is that the number of processes within the input
communicator \code{comm} does not fit \code{np0 x np1}. In this case the Cartesian communicator
is not generated. Therefore, might look like
\begin{lstlisting}
gc* Create two-dimensional process grid of size np[0] x np[1],
   if possible *gc
if( pfft_create_procmesh_2d(MPI_COMM_WORLD, np[0], np[1],
        &comm_cart_2d) )
{
  pfft_fprintf(MPI_COMM_WORLD, stderr,
      "Error: This test file only works with %d processes.\n",
      np[0]*np[1]);
  MPI_Finalize();
  return 1;
}
\end{lstlisting}
Hereby, we print the error message with the help of the PFFT library function
\begin{lstlisting}
void pfft_fprintf(
    MPI_Comm comm, FILE *stream, const char *format, ...);
\end{lstlisting}
This function is similar to the standard C function \code{fprintf} with the exception, that all messages are restricted
to the process of rank $0$ within the given communicator.

\subsection{Inplace transforms}
Similar to FFTW, PFFT is able to compute parallel FFTs completely in place, which means that beside some
constant buffers, no second data array beside the inputs is necessary. Especially, the global data communication
can be performed in place. As far as we know, there is no other parallel FFT library beside FFTW and PFFT that
supports this feature. 
This feature is enabled as soon as the pointer to the output array \code{out} is equal to the pointer to the input array \code{in}.
E.g., in Listing~\ref{lst:min_c2c} we would call
\begin{lstlisting}[firstnumber=34]
gc* Plan parallel forward FFT */
plan = pfft_plan_dft_3d(n, in, in, comm_cart_2d,
    PFFT_FORWARD, PFFT_TRANSPOSED_NONE);
\end{lstlisting}

\subsection{PFFT flags}
\begin{compactitem}
  \item \code{PFFT_TRANSPOSED_IN}
  \item \code{PFFT_TRANSPOSED_OUT}
  \item \code{PFFT_SHIFTED_IN}
  \item \code{PFFT_SHIFTED_OUT}
  \item \code{PFFT_ESTIMATE}
  \item \code{PFFT_MEASURE}
  \item \code{PFFT_PATIENT}
  \item \code{PFFT_EXHAUSIVE}
  \item \code{PFFT_NO_TUNE}
  \item \code{PFFT_TUNE}
  \item \code{PFFT_PRESERVE_INPUT}
  \item \code{PFFT_DESTROY_INPUT}
  \item \code{PFFT_BUFFERED_INPLACE}
\end{compactitem}


\section{Parallel data distribution}

\section{Three-dimensional FFTs with three-dimensional data decomposition}

\section{Arbitrary dimensional data decomposition}
Our first test program used a two-dimensional data decomposition of a three-dimensional data set.
Moreover, PFFT support the computation of any $d$-dimensional FFT with $r$-dimensional data decomposition
as long as $r\le d-1$. For example, one can use a one-dimensional data decomposition for any two- or higher-dimensional data set,
while the data set must be at least four-dimensional to fit to a three-dimensional data decomposition.
The case $r=d$ is not supported efficiently, since during the parallel computations
there is always at least one dimension that remains local, i.e., one dimensions stays non-decomposed.
However, for the case $d=r=3$ we offer some library

The dimensionality of the data decomposition is given by the dimension of the Cartesian communicator that
goes into the PFFT planing routines. Therefore, we present a generalization of communicator creation function
\begin{lstlisting}
int PX(create_procmesh)(
    int rnk, MPI_Comm comm, const int *np,
    MPI_Comm *comm_cart);
\end{lstlisting}
Hereby, the array \code{np} of length \code{rnk} gives the size of the Cartesian communicator \code{cart_comm}.


\newpage
Transposed $d$-dimensional array distributed on $c$-dimensional process mesh ($c<d$)
\begin{equation*}
  \frac{n_1}{P_0} \times \frac{n_2}{P_1} \times \hdots \times \frac{n_c}{P_{c-1}}  \times n_0 \times n_{c+1} \times \hdots \times n_{d-1}
\end{equation*}

Comparision of non-transposed (left) and transposed (right) $d$-dimensional array for $c$-dimensional process mesh ($c<d$)
\begin{equation*}
  n_0\times n_1\times \hdots n_{c-1} \times n_c \hdots \times n_{d-1} \Rightarrow n_1 \times \hdots n_{c-1} \times n_0 \times n_c \times \hdots \times n_{d-1}
\end{equation*}







% \code{n}
% \code{comm\_cart\_2d}





\section{Parallel FFT Frameworks}

\subsection{Forward Framework A (transposed input)}
\figurename{}~\ref{fig:fft_forw} lists the pseudo code of the parallel forward FFT framework.
\begin{figure}[ht]
  \begin{algorithmic}[1]
  %   \State\Comment{Calculate the serial FFTs row wise}
    \For{$t\gets0,\hdots,d-r-2$}
      \State $h_0 \gets \bigtimes_{s=0}^{r-1} N_sgcP_s \times \bigtimes_{s=r}^{d-2-t} N_s$
      \State $N   \gets N_{d-1-t}$
      \State $h_1 \gets \bigtimes_{s=d-t}^{d-1} \hat N_s \times h$
      \State $h_0 \times N \times h_1 \osetarrow{FFT} h_0 \times \hat N \times h_1$
    \EndFor
    \For{$t\gets 0,\hdots,r-1$}
      \State $h_0 \gets \bigtimes_{s=r-t}^{r-1} \hat N_{s+1}gcP_s \times \bigtimes_{s=0}^{r-t-1} N_s/P_s$
      \State $N   \gets N_{r-t}$
      \State $h_1 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $h_0 \times N \times h_1 \ousetarrow{FFT}{TO} \hat N \times h_0 \times h_1$
      \State
      \State $L_0 \gets \hat N_{r-t}$
      \State $h_0 \gets \bigtimes_{s=r-t}^{r-1}\hat N_{s+1}gcP_{s} \times \bigtimes_{s=0}^{r-t-2} N_s/P_s$
      \State $L_1 \gets N_{r-t-1}$
      \State $h_1 \gets 1$
      \State $h_2 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $P   \gets P_{r-t-1}$
%       \State \mbox{$L_0 \times h_0 \times L_1gcP \times h_1 \times h_2$} \mbox{$\ousetarrow{T}{TI} L_0/P \times h_0 \times L_1 \times h_1 \times h_2$}
      \State $L_0 \times h_0 \times L_1gcP \times h_1 \times h_2\ousetarrow{T}{TI} L_0/P \times h_0 \times L_1 \times h_1 \times h_2$
    \EndFor
    \State $h_0 \gets \bigtimes_{s=0}^{r-1}\hat N_{s+1}gcP_s$
    \State $N   \gets N_0$
    \State $h_1 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
    \State $h_0 \times N \times h_1 \osetarrow{FFT} h_0 \times \hat N \times h_1$
  \end{algorithmic}
  \caption{Parallel Forward FFT Framework}\label{fig:fft_forw}
\end{figure}

Within the first loop we use the serial FFT module~\eqref{eq:pruned_fft} to calculate
the one-dimensional (pruned) FFTs along the last $d-r-1$ array dimensions.
In the second loop we calculate $r$ one-dimensional pruned FFTs with transposed output~\eqref{eq:pruned_fft_to}
interleaved by global data transpositions with transposed input~\eqref{eq:gtransp_ti}.
Finally, a single non-transposed FFT~\eqref{eq:pruned_fft} must be computed to finish the full $d$-dimensional FFT.
The data decomposition of the output is then given by
\begin{equation*}
  \hat N_1gcP_0 \times \hdots \times \hat N_{r-2}/P_{r-1} \times \hat N_r \times \hdots \times \hat N_{d-1} \times h\,.
\end{equation*}
Note, that the dimensions of the output array are slightly transposed.

\subsection{Backward Framework C (transposed output)}
Now, the parallel backward FFT framework can be derived very easy since we only need to revert all the steps
of the forward framework. The backward framework starts with the output decomposition of the forward framework
\begin{equation*}
  \hat N_1gcP_0 \times \hdots \times \hat N_{r-2}/P_{r-1} \times \hat N_r \times \hdots \times \hat N_{d-1} \times h
\end{equation*}
and ends with the initial data decomposition
\begin{equation*}
  N_0gcP_0 \times \hdots \times N_{r-1}/P_{r-1} \times N_r \times \hdots \times N_{d-1} \times h\,.
\end{equation*}
\figurename{}~\ref{fig:fft_back} lists the parallel backward FFT framework in pseudo code.
\begin{figure}[ht]
  \begin{algorithmic}[1]
  %   \State\Comment{Calculate the serial FFTs row wise}
    \State $h_0 \gets \bigtimes_{s=0}^{r-1}\hat N_{s+1}gcP_s$
    \State $N   \gets \hat N_0$
    \State $h_1 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
    \State $h_0 \times \hat N \times h_1 \osetarrow{FFT} h_0 \times N \times h_1$
    \For{$t\gets r-1,\hdots,0$}
      \State $L_1 \gets \hat N_{r-t}$
      \State $h_1 \gets \bigtimes_{s=r-t}^{r-1}\hat N_{s+1}gcP_{s} \times \bigtimes_{s=0}^{r-t-2} N_s/P_s$
      \State $L_0 \gets N_{r-t-1}$
      \State $h_0 \gets 1$
      \State $h_2 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $P   \gets P_{r-t-1}$
      \State $L_1gcP \times h_1 \times L_0 \times h_0 \times h_2\ousetarrow{T}{TO} L_1 \times h_1 \times L_0/P \times h_0 \times h_2$
%       \State \mbox{$L_1gcP \times h_1 \times L_0 \times h_0 \times h_2$} \mbox{$\ousetarrow{T}{TO} L_1 \times h_1 \times L_0/P \times h_0 \times h_2$}
      \State
      \State $h_0 \gets \bigtimes_{s=r-t}^{r-1} \hat N_{s+1}gcP_s \times \bigtimes_{s=0}^{r-t-1} N_s/P_s$
      \State $N   \gets \hat N_{r-t}$
      \State $h_1 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $\hat N \times h_0 \times h_1 \ousetarrow{FFT}{TI} h_0 \times N \times h_1 $
    \EndFor
    \For{$t\gets d-r-2,\hdots,0$}
      \State $h_0 \gets \bigtimes_{s=0}^{r-1} N_sgcP_s \times \bigtimes_{s=r}^{d-2-t} N_s$
      \State $N   \gets \hat N_{d-1-t}$
      \State $h_1 \gets \bigtimes_{s=d-t}^{d-1} \hat N_s \times h$
      \State $h_0 \times \hat N \times h_1 \osetarrow{FFT} h_0 \times N \times h_1$
    \EndFor
  \end{algorithmic}
  \caption{Parallel Backward FFT Framework}\label{fig:fft_back}
\end{figure}

\newpage
\subsection{Backward Framework A (transposed input) - General \texorpdfstring{$r$, $d$}{r, d}}
\setlength{\arraycolsep}{2pt}
\begin{equation*}
  \begin{array}{cc}
    & \frac{\hat N_1}{P_0} \times \hdots \times \frac{\hat N_r}{P_{r-1}} \times \hat N_0 \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \ousetarrow{FFT}{TO} & N_0 \times \frac{\hat N_1}{P_0} \times \hdots \times \frac{\hat N_r}{P_{r-1}} \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \transposearrow{TI} & \frac{N_0}{P_0} \times \hat N_1 \times \frac{\hat N_2}{P_1} \times \hdots \times \frac{\hat N_r}{P_{r-1}} \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \ousetarrow{FFT}{TO} & N_1 \times \frac{N_0}{P_0} \times \frac{\hat N_2}{P_1} \times \hdots \times \frac{\hat N_r}{P_{r-1}} \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \vdots & \\
    \transposearrow{TI} & \frac{N_{r-1}}{P_{r-1}} \times \hdots \times \frac{N_0}{P_0} \times \hat N_r \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \ousetarrow{FFT}{\red{T}} & \frac{N_0}{P_0} \times \hdots \times \frac{N_{r-1}}{P_{r-1}} \times N_r \times \hat N_{r+1} \times \hdots \times \hat N_{d-1} \times h \\
    \ousetarrow{FFT}{} & \frac{N_0}{P_0} \times \hdots \times \frac{N_{r-1}}{P_{r-1}} \times N_r \times N_{r+1} \times \hat N_{r+2} \times \hdots \times \hat N_{d-1} \times h \\
    \vdots & \\
    \ousetarrow{FFT}{} & \frac{N_0}{P_0} \times \hdots \times \frac{N_{r-1}}{P_{r-1}} \times N_r \times N_{r+1} \times \hdots \times N_{d-1} \times h \\
  \end{array}
\end{equation*}

\newpage
\subsection{Backward Framework A (transposed input) for \texorpdfstring{$r=3$, $d=5$}{r=3 and d=5}}
\setlength{\arraycolsep}{2pt}
\begin{equation*}
  \begin{array}{cccc}
    & \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \hat N_0 \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \Big(N_0\Big) \times \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \left(\frac{N_0}{P_0}\right) \times \hat N_1 \times \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \left(N_1 \times \frac{N_0}{P_0}\right) \times \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \left(\frac{N_1}{P_1} \times \frac{N_0}{P_0}\right) \times \hat N_2 \times \left(\frac{\hat N_3}{P_2} \times \hat N_4 \times h \right)
    & \ousetarrow{FFT}{TO} & \left(N_2 \times \frac{N_1}{P_1} \times \frac{N_0}{P_0}\right) \times \left(\frac{\hat N_3}{P_2} \right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \frac{N_2}{P_2} \times \frac{N_1}{P_1} \times \frac{N_0}{P_0} \times \left(\hat N_3 \times \hat N_4 \times h \right)
    & \ousetarrow{FFT}{\red{T}} & \frac{N_0}{P_0} \times \frac{N_1}{P_1} \times \frac{N_2}{P_2} \times N_3 \times N_4 \times h
  \end{array}
\end{equation*}
Last step need more general interface to serial FFT module!

\subsection{Backward Framework B (transposed input and output) for \texorpdfstring{$r=3$, $d=5$}{r=3 and d=5}}
\setlength{\arraycolsep}{2pt}
\begin{equation*}
  \begin{array}{cccc}
    & \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \hat N_0 \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \Big(N_0\Big) \times \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & \hat N_1 \times \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times \left(\hat N_4 \times h\right)
    & \osetarrow{FFT} & \Big(N_1\Big) \times \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & \hat N_2 \times \left(\frac{\hat N_3}{P_2} \times \frac{N_0}{P_0} \times \frac{N_1}{P_1}\right) \times \left(\hat N_4 \times h\right)
    & \osetarrow{FFT} & \Big(N_2\Big) \times \left(\frac{\hat N_3}{P_2} \times \frac{N_0}{P_0} \times \frac{N_1}{P_1}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & \hat N_3 \times \left(\frac{N_0}{P_0} \times \frac{N_1}{P_1}  \times \frac{N_2}{P_2}\right) \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TI} & \frac{N_0}{P_0} \times \frac{N_1}{P_1}  \times \frac{N_2}{P_2} \times N_3 \times N_4 \times h
  \end{array}
\end{equation*}

\subsection{Forward Framework A (transposed input) for \texorpdfstring{$r=3$, $d=5$}{r=3 and d=5}}
\setlength{\arraycolsep}{2pt}
\begin{equation*}
  \begin{array}{cccc}
    & \left(\frac{N_0}{P_0} \times \frac{N_1}{P_1}  \times \frac{N_2}{P_2}\right) \times N_3 \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \left(\hat N_3 \times \frac{N_0}{P_0} \times \frac{N_1}{P_1}\right) \times \left(\frac{N_2}{P_2}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \left(\frac{\hat N_3}{P_2} \times \frac{N_0}{P_0} \times \frac{N_1}{P_1}\right)  \times N_2 \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \left(\hat N_2 \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times \left(\frac{N_1}{P_1}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times N_1 \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \left(\hat N_1 \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \left(\frac{N_0}{P_0}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TI} & \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times N_0 \times \left(\hat N_4 \times h\right)
    & \osetarrow{FFT} & \frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \hat N_0 \times \hat N_4 \times h
  \end{array}
\end{equation*}

\subsection{Forward Framework B (transposed input and output) for \texorpdfstring{$r=3$, $d=5$}{r=3 and d=5}}
\setlength{\arraycolsep}{2pt}
\begin{equation*}
  \begin{array}{cccc}
    & \left(\frac{N_0}{P_0} \times \frac{N_1}{P_1}  \times \frac{N_2}{P_2}\right) \times N_3 \times \left(N_4 \times h\right)
    & \ousetarrow{FFT}{TO} & \left(\hat N_3 \times \frac{N_0}{P_0} \times \frac{N_1}{P_1}\right)  \times \left(\frac{N_2}{P_2}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & N_2 \times \left(\frac{\hat N_3}{P_2} \times \frac{N_0}{P_0} \times \frac{N_1}{P_1} \right) \times \left( \hat N_4 \times h\right)
    & \osetarrow{FFT} & \left(\hat N_2 \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times \left(\frac{N_1}{P_1}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & N_1 \times \left(\frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \frac{N_0}{P_0}\right) \times \left( \hat N_4 \times h\right)
    & \osetarrow{FFT} & \left(\hat N_1 \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \left(\frac{N_0}{P_0}\right) \times \left(\hat N_4 \times h\right) \\
    \transposearrow{TIO} & N_0 \times \left(\frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2}\right) \times \left(\hat N_4 \times h\right)
    & \ousetarrow{FFT}{TI} & \frac{\hat N_1}{P_0} \times \frac{\hat N_2}{P_1} \times \frac{\hat N_3}{P_2} \times \hat N_0 \times \hat N_4 \times h
  \end{array}
\end{equation*}

\newpage
\subsection{Backward Framework A (transposed input) - The general form}
\figurename{}~\ref{fig:fft_back_A} lists the parallel backward FFT framework A in pseudo code.
\begin{figure}[ht]
  \begin{algorithmic}[1]
  %   \State\Comment{Calculate the serial FFTs row wise}
    \For{$t\gets 0,\hdots,r-1$}
      \State $h_0 \gets \bigtimes_{s=t}^{r-1}\hat N_{s+1}gcP_s$
      \State $N   \gets \hat N_t$
      \State $h_1 \gets \bigtimes_{s=t+1}^{r-1} \hat N_{s+1}gc P_{s} \times \hat N_{r}  \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $h_0 \times \hat N \times h_1 \ousetarrow{FFT}{TO} N \times h_0 \times h_1$
      \State
      \State $L_1 \gets N_t$
      \State $h_1 \gets \bigtimes_{s=0}^{t-1} N_{s}gcP_{s}$
      \State $L_0 \gets \hat N_{t+1}$
      \State $h_0 \gets \bigtimes_{s=t+1}^{r-1}\hat N_{s+1}gcP_{s}$
      \State $h_2 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
      \State $P   \gets P_{t}$
      \State $L_1 \times h_1 \times L_0gcP \times h_0 \times h_2\ousetarrow{T}{TI} L_1/P \times h_1 \times L_0 \times h_0 \times h_2$
    \EndFor
    \State $h_0 \gets \bigtimes_{s=r-t}^{r-1} \hat N_{s+1}gcP_s \times \bigtimes_{s=0}^{r-t-1} N_s/P_s$
    \State $N   \gets \hat N_{r-t}$
    \State $h_1 \gets \bigtimes_{s=r+1}^{d-1} \hat N_s \times h$
    \State $\hat N \times h_0 \times h_1 \ousetarrow{FFT}{TI} h_0 \times N \times h_1 $
    \For{$t\gets d-r-2,\hdots,0$}
      \State $h_0 \gets \bigtimes_{s=0}^{r-1} N_sgcP_s \times \bigtimes_{s=r}^{d-2-t} N_s$
      \State $N   \gets \hat N_{d-1-t}$
      \State $h_1 \gets \bigtimes_{s=d-t}^{d-1} \hat N_s \times h$
      \State $h_0 \times \hat N \times h_1 \osetarrow{FFT} h_0 \times N \times h_1$
    \EndFor
  \end{algorithmic}
  \caption{Parallel Backward FFT Framework A}\label{fig:fft_back_A}
\end{figure}


\begin{compactitem}
  \item[\mybox] \verb+#include <complex.h> #include <pfft.h>+
\end{compactitem}


\section{Library reference}
\begin{lstlisting}
void pfft_init(void);
void pfft_cleanup(void);
\end{lstlisting}

\begin{lstlisting}
void *pfft_malloc(size_t n);
double *pfft_alloc_real(size_t n);
pfft_complex *pfft_alloc_complex(size_t n);
void pfft_free(void *p);
\end{lstlisting}

\begin{lstlisting}
void pfft_execute(pfft_plan fftplan);
void pfft_destroy_plan(pfft_plan fftplan);
\end{lstlisting}

\begin{lstlisting}
void pfft_init_input_c2c_3d(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    pfft_complex *data);
void PX(init_input_c2c)(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    pfft_complex *data);
\end{lstlisting}

\begin{lstlisting}
void PX(init_input_r2c_3d)(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    double *data);
void PX(init_input_r2c)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
      double *data);

void PX(init_input_r2r_3d)(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    double *data);
void PX(init_input_r2r)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
      double *data);

double PX(check_output_c2c_3d)(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const pfft_complex *data, MPI_Comm comm);
double PX(check_output_c2c)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
      const pfft_complex *data, MPI_Comm comm);

double PX(check_output_c2r_3d)(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const double *data, MPI_Comm comm);
double PX(check_output_c2r)(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const double *data, MPI_Comm comm);

 PFFT_EXTERN double PX(check_output_r2r_3d)(
    const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_n_start,
    const double *data, MPI_Comm comm);
double PX(check_output_r2r)(
    int rnk_n, const ptrdiff_t *n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    const double *data, MPI_Comm comm);

ptrdiff_t PX(local_size_dft_3d)(
      const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_dft_r2c_3d)(
      const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_dft_c2r_3d)(
      const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_r2r_3d)(
      const ptrdiff_t *n, MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start, ptrdiff_t *local_no, ptrdiff_t *local_o_start);

ptrdiff_t PX(local_size_dft)(
      int rnk, const ptrdiff_t *n,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_dft_r2c)(
      int rnk, const ptrdiff_t *n,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_dft_c2r)(
      int rnk, const ptrdiff_t *n,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_r2r)(
      int rnk_n, const ptrdiff_t *n,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);

ptrdiff_t PX(local_size_many_dft)(
      int rnk, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_many_dft_r2c)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_many_dft_c2r)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);
ptrdiff_t PX(local_size_many_r2r)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      MPI_Comm comm_cart, unsigned pfft_flags,
      ptrdiff_t *local_ni, ptrdiff_t *local_i_start,
      ptrdiff_t *local_no, ptrdiff_t *local_o_start);


PX(plan) PX(plan_dft_3d)(
      const ptrdiff_t *n, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_dft_r2c_3d)(
      const ptrdiff_t *n, double *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_dft_c2r_3d)(
      const ptrdiff_t *n, pfft_complex *in, double *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_r2r_3d)(
      const ptrdiff_t *n, double *in, double *out, MPI_Comm comm_cart,
      const PX(r2r_kind) *kinds, unsigned pfft_flags);

PX(plan) PX(plan_dft)(
      int rnk, const ptrdiff_t *n, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_dft_r2c)(
      int rnk_n, const ptrdiff_t *n, double *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_dft_c2r)(
      int rnk_n, const ptrdiff_t *n, pfft_complex *in, double *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
  PX(plan) PX(plan_r2r)(
    int rnk_n, const ptrdiff_t *n, double *in, double *out, MPI_Comm comm_cart,
    const PX(r2r_kind) *kinds, unsigned pfft_flags);

PX(plan) PX(plan_many_dft)(
      int rnk, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_many_dft_r2c)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      double *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_many_dft_c2r)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      pfft_complex *in, double *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);
PX(plan) PX(plan_many_r2r)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      double *in, double *out, MPI_Comm comm_cart,
      const PX(r2r_kind) *kinds, unsigned pfft_flags);

PX(plan) PX(plan_many_dft_skipped)(
      int rnk, const ptrdiff_t *n, const ptrdiff_t *ni, const ptrdiff_t *no,
      ptrdiff_t howmany, const ptrdiff_t *iblock, const ptrdiff_t *oblock,
      const int *skip_trafos, pfft_complex *in, pfft_complex *out, MPI_Comm comm_cart,
      int sign, unsigned pfft_flags);


ptrdiff_t PX(prod_ptrdiff_t)(
      int d, const ptrdiff_t *vec);
ptrdiff_t PX(sum_ptrdiff_t)(
      int d, const ptrdiff_t *vec);
int PX(equal_ptrdiff_t)(
      int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2);
void PX(vcopy_ptrdiff_t)(
      int d, const ptrdiff_t *vec1,
      ptrdiff_t *vec2);
void PX(vadd_ptrdiff_t)(
      int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2,
      ptrdiff_t *sum);
void PX(vsub_ptrdiff_t)(
      int d, const ptrdiff_t *vec1, const ptrdiff_t *vec2,
      ptrdiff_t *sum);

  void PX(apr_complex_3d)(
      const pfft_complex *data, const ptrdiff_t *local_n,
      const ptrdiff_t *local_start, const char *name, MPI_Comm comm);
  void PX(apr_complex_permuted_3d)(
      const pfft_complex *data, const ptrdiff_t *local_n,
      const ptrdiff_t *local_start,
      int perm0, int perm1, int perm2, const char *name, MPI_Comm comm);

void PX(get_args)(
      int argc, char **argv, const char *name,
      int neededArgs, unsigned type,
      void *parameter);


void PX(reset_timer)(
      PX(plan) ths);
PX(timer) PX(get_timer)(
      const PX(plan) ths);
void PX(print_average_timer)(
      const PX(plan) ths, MPI_Comm comm);
void PX(print_average_timer_adv)(
      const PX(plan) ths, MPI_Comm comm);
void PX(write_average_timer)(
      const PX(plan) ths, const char *name, MPI_Comm comm);
void PX(write_average_timer_adv)(
      const PX(plan) ths, const char *name, MPI_Comm comm);

PX(timer) PX(copy_timer)(
      const PX(timer) orig);
void PX(average_timer)(
      PX(timer) ths);
PX(timer) PX(add_timers)(
      const PX(timer) sum1, const PX(timer) sum2);
PX(timer) PX(reduce_max_timer)(
      const PX(timer) ths, MPI_Comm comm);
double* PX(convert_timer2vec)(
      const PX(timer) ths);
PX(timer) PX(convert_vec2timer)(
      const double *times);
void PX(destroy_timer)(
      PX(timer) ths);



void PX(vfprintf)(
      MPI_Comm comm, FILE *stream, const char *format, va_list ap);
void PX(fprintf)(
      MPI_Comm comm, FILE *stream, const char *format, ...);
void PX(printf)(
      MPI_Comm comm, const char *format, ...);

int PX(create_procmesh)(
      int rnk, MPI_Comm comm, const int *np,
      MPI_Comm *comm_cart);
int PX(create_procmesh_2d)(
      MPI_Comm comm, int np0, int np1,
      MPI_Comm *comm_cart_2d);



ptrdiff_t PX(local_size_gc_3d)(
    const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
ptrdiff_t PX(local_size_gc)(
    int rnk_n, const ptrdiff_t *local_n, const ptrdiff_t *local_start,
    ptrdiff_t alloc_local, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);
ptrdiff_t PX(local_size_many_gc)(
    int rnk_n, const ptrdiff_t *local_n, const ptrdiff_t *local_start, ptrdiff_t alloc_local,
    ptrdiff_t howmany, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    ptrdiff_t *local_ngc, ptrdiff_t *local_gc_start);

PX(gcplan) PX(plan_rgc_3d)(
      const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
      double *data, MPI_Comm comm_cart, unsigned gc_flags);
PX(gcplan) PX(plan_cgc_3d)(
    const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
    pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
PX(gcplan) PX(plan_rgc)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
      double *data, MPI_Comm comm_cart, unsigned gc_flags);
PX(gcplan) PX(plan_cgc)(
      int rnk_n, const ptrdiff_t *n, const ptrdiff_t *gc_below, const ptrdiff_t *gc_above,
      pfft_complex *data, MPI_Comm comm_cart, unsigned gc_flags);
PX(gcplan) PX(plan_many_rgc)(
      int rnk_n, const ptrdiff_t *n, ptrdiff_t howmany, const ptrdiff_t *block,
      const ptrdiff_t *gc_below, const ptrdiff_t *gc_above, double *data, MPI_Comm comm_cart,
      unsigned gc_flags);
PX(gcplan) PX(plan_many_cgc)(
      int rnk_n, const ptrdiff_t *n, ptrdiff_t howmany, const ptrdiff_t *block,
      const ptrdiff_t *gc_below, const ptrdiff_t *gc_above, pfft_complex *data, MPI_Comm comm_cart,
      unsigned gc_flags);

void PX(exchange)(
      PX(gcplan) ths);
void PX(reduce)(
      PX(gcplan) ths);
void PX(destroy_gcplan)(
      PX(gcplan) ths);

void PX(reset_gctimers)(
      PX(gcplan) ths);
PX(gctimer) PX(get_gctimer_exg)(
      const PX(gcplan) ths);
PX(gctimer) PX(get_gctimer_red)(
      const PX(gcplan) ths);
void PX(print_average_gctimer)(
      const PX(gcplan) ths, MPI_Comm comm);
void PX(print_average_gctimer_adv)(
      const PX(gcplan) ths, MPI_Comm comm);
void PX(write_average_gctimer)(
      const PX(gcplan) ths, const char *name, MPI_Comm comm);
void PX(write_average_gctimer_adv)(
      const PX(gcplan) ths, const char *name, MPI_Comm comm);

PX(gctimer) PX(copy_gctimer)(
      const PX(gctimer) orig);
void PX(average_gctimer)(
      PX(gctimer) ths);
PX(gctimer) PX(add_gctimers)(
      const PX(gctimer) sum1, const PX(gctimer) sum2);
PX(gctimer) PX(reduce_max_gctimer)(
      const PX(gctimer) ths, MPI_Comm comm);
void PX(convert_gctimer2vec)(
      const PX(gctimer) ths, double *times);
PX(gctimer) PX(convert_vec2gctimer)(
      const double *times);
void PX(destroy_gctimer)(
      PX(gctimer) ths);
\end{lstlisting}

